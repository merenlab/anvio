# -*- coding: utf-8

import argparse
import os
import pandas as pd

import anvio
import anvio.utils as u
import anvio.workflows as w

from anvio.workflows.trnaseq import TRNASeqWorkflow
from anvio.errors import ConfigError


__author__ = "Samuel Miller"
__copyright__ = "Copyright 2017, The anvio Project"
__credits__ = []
__license__ = "GPL 3.0"
__version__ = anvio.__version__
__maintainer__ = "Samuel Miller"
__email__ = "samuelmiller10@gmail.com"


M = TRNASeqWorkflow(argparse.Namespace(config=config))
M.init()
dirs_dict = M.dirs_dict


rule trnaseq_workflow_target_rule:
    """The target rule for the workflow"""
    input: M.target_files


rule make_iu_input:
    """Create an Illumina-utils samples file from samples_txt (run once)."""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "make_iu_input.log")
    # The input file is marked "ancient" to prevent Snakemake from rerunning this step
    # when the samples_txt file has been changed.
    input: ancient(M.get_param_value_from_config(['samples_txt']))
    output: os.path.join(dirs_dict['QC_DIR'], "iu_samples_input.txt")
    threads: M.T('make_iu_input')
    resources: nodes = M.T('make_iu_input')
    run:
        samples_txt_df = pd.read_csv(input[0], sep='\t', index_col=False)
        iu_samples_input_df = samples_txt_df[['r1', 'r2']] # r1 and r2 filepaths
        iu_samples_input_df['sample'] = samples_txt_df['sample'] + '_' + samples_txt_df['split'] # e.g., sample1_demethylase, sample1_untreated
        iu_samples_input_df[['sample', 'r1', 'r2']].to_csv(output[0], sep='\t', index=False) # reorder columns


rule iu_gen_configs:
    """Create an Illumina-utils config file for each sample+split pair from the Illumina-utils samples file (run once)."""
    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "iu_gen_configs.log")
    # The input file is marked "ancient" to prevent Snakemake from rerunning this step
    # when the iu_samples_input file has been changed.
    input: ancient(os.path.join(dirs_dict["QC_DIR"], "iu_samples_input.txt"))
    output: expand(os.path.join(dirs_dict["QC_DIR"], "{sample_split_prefix}.ini"), sample_split_prefix=M.sample_split_prefixes)
    params:
        DIR = dirs_dict['QC_DIR'],
        r1_prefix = M.get_rule_param('iu_gen_configs', '--r1-prefix'), # regex-formatted adapter tag at the beginning of the forward read, e.g., "^...AAGT"
        r2_prefix = M.get_rule_param('iu_gen_configs', '--r2-prefix') # regex-formatted adapter tag at the beginning of the reverse read
    threads: M.T('iu_gen_configs')
    resources: nodes = M.T('iu_gen_configs')
    shell: "iu-gen-configs {input} -o {params.DIR} {params.r1_prefix} {params.r2_prefix} >> {log} 2>&1"


rule iu_merge_pairs:
    """Merge paired-end reads using Illumina-utils."""
    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{sample_split_prefix}-iu_merge_pairs.log")
    input: os.path.join(dirs_dict['QC_DIR'], "{sample_split_prefix}.ini")
    output:
        merged = temp(os.path.join(dirs_dict['QC_DIR'], "{sample_split_prefix}_MERGED")) if M.gzip_iu_merge_pairs_output else os.path.join(dirs_dict['QC_DIR'], "{sample_split_prefix}_MERGED"),
        stats = os.path.join(dirs_dict['QC_DIR'], "{sample_split_prefix}_STATS")
    params:
        marker_gene_stringent = M.get_rule_param('iu_merge_pairs', '--marker-gene-stringent'), # allows for both full and partial overlap of inserts, by default trimming trailing adapters following fully overlapping inserts
        max_num_mismatches = M.get_rule_param('iu_merge_pairs', '--max-num-mismatches'),
        report_r1_prefix = M.get_rule_param('iu_merge_pairs', '--report-r1-prefix'), # flag for reporting the actual sequence of an adapter tag at the beginning of the forward read, which should be specified in iu_gen_configs
        report_r2_prefix = M.get_rule_param('iu_merge_pairs', '--report-r2-prefix') # flag for reporting the actual sequence of an adapter tag at the beginning of the reverse read
    threads: M.T('iu_merge_pairs')
    resources: nodes = M.T('iu_merge_pairs')
    run:
        shell("iu-merge-pairs {input} {params.marker_gene_stringent} {params.max_num_mismatches} {params.report_r1_prefix} {params.report_r2_prefix} --num-threads {threads} >> {log} 2>&1")
        if M.gzip_iu_merge_pairs_output:
            shell("gzip -k {output.merged} >> {log} 2>&1")
            shell("gzip %s >> {log} 2>&1" % os.path.join(dirs_dict['QC_DIR'], wildcards.sample_split_prefix + "_FAILED"))
            shell("gzip %s >> {log} 2>&1" % os.path.join(dirs_dict['QC_DIR'], wildcards.sample_split_prefix + "_FAILED_WITH_Ns"))
            if params.report_r1_prefix:
                shell("gzip %s >> {log} 2>&1" % os.path.join(dirs_dict['QC_DIR'], wildcards.sample_split_prefix + "_MERGED_R1_PREFIX"))
            if params.report_r2_prefix:
                shell("gzip %s >> {log} 2>&1" % os.path.join(dirs_dict['QC_DIR'], wildcards.sample_split_prefix + "_MERGED_R2_PREFIX"))


rule gen_qc_report:
    """Report all quality control statistics (run once)."""
    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "gen_qc_report.log")
    input: expand(os.path.join(dirs_dict['QC_DIR'], "{sample_split_prefix}_STATS"), sample_split_prefix=M.sample_split_prefixes)
    output: report = os.path.join(dirs_dict['QC_DIR'], "qc_report.txt") # optional target file that triggers Illumina-utils QC steps
    threads: M.T('gen_qc_report')
    resources: nodes = M.T('gen_qc_report')
    run:
        report_dict = {}
        headers = []
        for i, filename in enumerate(input):
            sample_split = os.path.basename(filename).split("_STATS")[0]
            report_dict[sample_split] = {}
            file_headers = []
            with open(filename) as f:
                firstline = True
                for line in f:
                    if line == '\n':
                        break
                    line_frags = line.rstrip().split(' ...')
                    header = line_frags[0]
                    file_headers.append(header)
                    number = line_frags[1].split('\t')[1]
                    report_dict[sample_split][header] = number
            if i == 0:
                headers = file_headers
            else:
                if file_headers != headers:
                    raise ConfigError("The difference in output headers between STATS files "
                                      "indicates an inconsistency in how files were processed by 'iu-merge-pairs'. "
                                      "These files, for example, have a difference between their headers: "
                                      "%s and %s" % (input[i], input[i - 1]))
        u.store_dict_as_TAB_delimited_file(report_dict, output.report, headers=['sample + split'] + headers)


def gunzip_fasta(fasta, log):
    fasta_gunzipped = os.path.splitext(fasta)[0]
    shell("gunzip < %s > %s 2>> %s" % (fasta, fasta_gunzipped, log))
    return fasta_gunzipped


rule anvi_reformat_fasta:
    """Reformat the FASTA file with Anvi\'o-compliant deflines."""
    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{sample_split_prefix}-reformat_fasta.log")
    input: M.get_input_for_anvi_reformat_fasta
    output:
        fasta = temp(os.path.join(dirs_dict['QC_DIR'], "{sample_split_prefix}-reformatted.fasta")) if M.gzip_anvi_reformat_fasta_output else os.path.join(dirs_dict['QC_DIR'], "{sample_split_prefix}-reformatted.fasta"),
        report = os.path.join(dirs_dict['QC_DIR'], "{sample_split_prefix}-reformat_report.txt") # optional target file that triggers reformatting
    threads: M.T('anvi_reformat_fasta')
    resources: nodes = M.T('anvi_reformat_fasta')
    params:
        simplify_names = M.get_rule_param('anvi_reformat_fasta', '--simplify-names')
    run:
        if input[0].endswith('.gz'):
            decompressed_fasta = gunzip_fasta(input[0], log[0])
        else:
            decompressed_fasta = input[0]
        shell("anvi-script-reformat-fasta -o {output.fasta} {params.simplify_names} --report-file {output.report} %s >> {log} 2>&1" % decompressed_fasta)
        if input[0].endswith('.gz'):
            os.remove(decompressed_fasta)
        if M.gzip_anvi_reformat_fasta_output:
            shell("gzip -k {output.fasta} >> {log} 2>&1")


rule anvi_trnaseq:
    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{sample_split_prefix}-anvi_trnaseq.log")
    input: M.get_input_for_anvi_trnaseq
    output:
        uniqued_nontrna_supplement = os.path.join(os.path.join(dirs_dict['IDENT_DIR'], "{sample_split_prefix}"), "{sample_split_prefix}-UNIQUED_NONTRNA.txt"),
        trimmed_ends_supplement = os.path.join(os.path.join(dirs_dict['IDENT_DIR'], "{sample_split_prefix}"), "{sample_split_prefix}-TRIMMED_ENDS.txt")
    threads: M.T('anvi_trnaseq')
    resources: nodes = M.T('anvi_trnaseq')
    params:
        overwrite_output_destinations = M.get_rule_param('anvi_trnaseq', '--overwrite-output-destinations'),
        description = M.get_rule_param('anvi_trnaseq', '--description'),
        skip_fasta_check = M.get_rule_param('anvi_trnaseq', '--skip-fasta-check'),
        write_checkpoints = M.get_rule_param('anvi_trnaseq', '--write-checkpoints'),
        load_checkpoint = M.get_rule_param('anvi_trnaseq', '--load-checkpoint'),
        write_buffer_size = M.get_rule_param('anvi_trnaseq', '--write-buffer-size'),
        alignment_target_chunk_size = M.get_rule_param('anvi_trnaseq', '--alignment-target-chunk-size'),
        fragment_mapping_query_chunk_length = M.get_rule_param('anvi_trnaseq', '--fragment-mapping-query-chunk-length'),
        feature_param_file = M.get_rule_param('anvi_trnaseq', '--feature-param-file'),
        min_length_long_fiveprime = M.get_rule_param('min_length_long_fiveprime', '--min-length-long-fiveprime'),
        min_trna_fragment_size = M.get_rule_param('anvi_trnaseq', '--min-trna-fragment-size'),
        agglomeration_max_mismatch_freq = M.get_rule_param('anvi_trnaseq', '--agglomeration-max-mismatch-freq'),
        max_deletion_size = M.get_rule_param('anvi_trnaseq', '--max-deletion-size'),
        alignment_progress_interval = M.get_rule_param('anvi_trnaseq', '--alignment-progress-interval'),
        agglomeration_progress_interval = M.get_rule_param('anvi_trnaseq', '--agglomeration-progress-interval')
    run:
        if input[0].endswith('.gz'):
            decompressed_fasta = gunzip_fasta(input[0], log[0])
        else:
            decompressed_fasta = input[0]
        output_dir = os.path.join(dirs_dict['IDENT_DIR'], wildcards.sample_split_prefix)
        shell("anvi-trnaseq --fasta-file %s --project-name {wildcards.sample_split_prefix} --output-dir %s {params.overwrite_output_destinations} {params.description} {params.skip_fasta_check} {params.write_checkpoints} {params.load_checkpoint} {params.write_buffer_size} {params.alignment_target_chunk_size} {params.fragment_mapping_query_chunk_length} {params.feature_param_file} {params.min_length_long_fiveprime} {params.min_trna_fragment_size} {params.agglomeration_max_mismatch_freq} {params.max_deletion_size} {params.alignment_progress_interval} {params.agglomeration_progress_interval} -T {threads} >> {log} 2>&1" % (decompressed_fasta, output_dir))
        if input[0].endswith('.gz'):
            os.remove(decompressed_fasta)


if 'workflows/trnaseq' in workflow.included[0]:
    # Check if all program dependencies are met.
    # For this line to be effective, there should be an initial dry run step --
    # which is the default behavior of `WorkflowSuperClass`, so you are most likely covered.
    M.check_workflow_program_dependencies(workflow, dont_raise=True)