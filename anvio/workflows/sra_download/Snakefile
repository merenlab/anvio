# -*- coding: utf-8
import os
import json
import anvio
import argparse
import pandas as pd

from anvio.errors import ConfigError
from anvio.workflows.sra_download import SRADownloadWorkflow

__author__ = "Matthew S. Schechter"
__copyright__ = "Copyright 2017, The anvio Project"
__credits__ = ['mschecht']
__license__ = "GPL 3.0"
__version__ = anvio.__version__
__maintainer__ = "Matthew S. Schechter"
__email__ = "mschechter@uchicago.edu"


"""
This Snakemake workflow downloads and compresses sequencing data from the NCBI Sequence Read Archive (SRA).

Software requirements:
- [The SRA toolkit](https://github.com/ncbi/sra-tools): prefetch, fasterq_dump
- [pigz](https://zlib.net/pigz/)

Briefly, the prefetch rule retrieves data from the SRA and stores it as {accession}.sra files.
Next, fasterq_dump rule extracts the FASTQ files from the prefetched SRA file.
Finally, the pigz rule gzips the FASTQ files.

The input for this workflow is a file of SRA accessions e.g.

$ cat SRA_accession_list.txt
ERR6450080
ERR6450081
SRR5965623
"""


M = SRADownloadWorkflow(argparse.Namespace(config=config))
M.init()

dirs_dict = M.dirs_dict

rule SRA_DOWNLOAD_WORKFLOW_target_rule:
    """The rule creates all target files for the Snakefile"""
    input: M.target_files


rule prefetch:
    """Prefetch data from the Sequence Read Archive (SRA).

    Inputs:
        None

    Outputs:
        SRA_TMP: File in the SRA_prefetch directory with the name {accession}.sra or {accession}.sralite

    Params:
        SRA_OUTPUT_DIR: Output directory for the prefetched data
        MAX_SIZE: Maximum size of the SRA file to download

    Threads:
        The number of threads to use is specified by the prefetch variable

    NOTES:
    - This is the first rule of the workflow
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{accession}_prefetch.log")
    input:
    output:
        SRA_TMP = os.path.join(dirs_dict['SRA_prefetch'], "{accession}", "{accession}.sra")
    params:
        SRA_OUTPUT_DIR = os.path.join(dirs_dict['SRA_prefetch']),
        MAX_SIZE = M.get_param_value_from_config(['prefetch', '--max-size'])
    threads: M.T('prefetch')
    run:
        shell("prefetch {wildcards.accession} --output-directory {params.SRA_OUTPUT_DIR} --max-size {params.MAX_SIZE} >> {log} 2>&1")

rule check_md5sum:
    """Curl the md5sum file from the SRA FTP site"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{accession}_check_md5sum.log")
    input:
        sra_file = os.path.join(dirs_dict['SRA_prefetch'], "{accession}", "{accession}.sra")
    params:
        md5sum = os.path.join(dirs_dict['SRA_prefetch'], "{accession}", "{accession}.json")
    output:
        md5sum_DONE = touch(os.path.join(dirs_dict['SRA_prefetch'], "{accession}", "{accession}.done"))
    threads: M.T('check_md5sum')
    run:
        # Get the md5sum from the SRA FTP site
        shell("curl 'https://locate.ncbi.nlm.nih.gov/sdl/2/retrieve?filetype=run&location-type=forced&location=s3.us-east-1&accept-charges=aws&acc={wildcards.accession}' --output {params.md5sum} >> {log} 2>&1")

        # Get the expected md5sum hash
        with open(params.md5sum) as sra_metadata_file:
            sra_metadata_dict = json.load(sra_metadata_file)
            expected_md5 = sra_metadata_dict['result'][0]['files'][0]['md5']

        def calculate_md5(file_path):
            """Calculate the md5sum of a file"""
            import hashlib
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()

        calculated_md5 = calculate_md5(input.sra_file)

        # Compare expected and calculated md5 and log the result
        log_path = str(log)
        with open(log_path, 'a') as log_file:
            if calculated_md5 == expected_md5:
                log_file.write(f"Checksums match: {calculated_md5}\n")
            else:
                error_message = f"Checksums do not match: calculated {calculated_md5}, expected {expected_md5}"
                log_file.write(error_message)
                raise ValueError(error_message)


rule fasterq_dump:
    """Use fasterq-dump to extract FASTQ file(s) from an SRA prefetch *.sra
    Using the flag --split-3. For more information about fasterq-dump:
    https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump

    Inputs:
    - SRA file from the output of the prefetch rule

    Outputs:
    - Three possible FASTQ files: single reads, and/or R1 and R2.

    Params:
    - SRA_INPUT_DIR: directory containing the SRA file
    - OUTPUT_DIR: directory to write the FASTQ file(s)

    Threads:
    - Number of threads specified in the M object
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{accession}_fasterq_dump.log")
    input: ancient(rules.check_md5sum.output.md5sum_DONE)
    output:
        FASTERQDUMP_DONE = touch(os.path.join(dirs_dict['FASTAS'], "{accession}-fasterq-dump.done")),
        FASTERQDUMP_TEMP = temp(directory("FASTERQDUMP_TEMP/{accession}"))
    params:
        SRA_INPUT_DIR = os.path.join(dirs_dict['SRA_prefetch'], "{accession}"),
        OUTPUT_DIR = dirs_dict['FASTAS']
    threads: M.T('fasterq_dump')
    run:
        shell("fasterq-dump {params.SRA_INPUT_DIR} -t {output.FASTERQDUMP_TEMP} --outdir {params.OUTPUT_DIR} --split-3 --verbose --progress --threads {threads} >> {log} 2>&1")

        # Check if fasterq-dump encountered a Disk quota exceeded error
        error_message = "Disk quota exceeded"
        log_path = str(log)
        with open(log_path, "r") as log_file:
            log_contents = log_file.read()
            if error_message in log_contents:
                raise Exception("fasterq-dump encountered a Disk quota exceeded when processing {wildcards.accession}")


rule pigz:
    """Compress FASTQ file(s) using pigz in parallel!

    Inputs:
    - FASTQ file(s) from the output of the fasterq_dump rule

    Outputs:
    - gzipped FASTQ file(s) in the FASTAS directory

    Params:
    - READS: prefix of the FASTQ file(s) in the FASTAS directory

    Threads:
    - Number of threads specified in the M object

    example:
        pigz --processes 8 --verbose 02_FASTA/ERR6450080*.fastq >> 00_LOGS/ERR6450080_pigz.log 2>&1
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{accession}_pigz.log")
    input: ancient(rules.fasterq_dump.output.FASTERQDUMP_DONE)
    output: touch(os.path.join(dirs_dict['FASTAS'], "{accession}-pigz.done"))
    params:
        READS = os.path.join(dirs_dict['FASTAS'], "{accession}*.fastq")
    threads: M.T('pigz')
    run:
        shell("pigz --processes {threads} --verbose {params.READS} >> {log} 2>&1")


rule generate_samples_txt:
    """Create samples-txt files for paired-end reads and single reads samples

    Inputs:
    - gziped FASTQ file(s) from the output of the pigz rule

    Outputs:
    - samples.txt and samples_single_reads.txt in the FASTAS directory

    Params:
    - ACCESSION: list of accessions
    - OUTPUT_DIR: directory to write the samples.txt file

    Threads:
    - Number of threads specified in the M object
    """

    version:1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "generate_samples_txt.log")
    input: expand(os.path.join(dirs_dict['FASTAS'], "{accession}-pigz.done"), accession=M.accessions_list)
    output: touch(os.path.join(dirs_dict['FASTAS'], "generate_samples_txt.done"))
    params:
        ACCESSION = M.accessions_list,
        OUTPUT_DIR = dirs_dict['FASTAS']
    run:

        paired_reads = []
        single_reads = []

        for sample in params.ACCESSION:
            if os.path.exists(os.path.join(params.OUTPUT_DIR, "".join([sample, "_1.fastq.gz"]))):
                paired_reads.append(sample)
            elif os.path.exists(os.path.join(params.OUTPUT_DIR, "".join([sample, ".fastq.gz"]))):
                single_reads.append(sample)
            else:
                raise ConfigError(f"Looks like sample {sample} doesn't have the expected output(s) format.")

        if paired_reads:
            with open("samples.txt", 'w') as f:
                f.write("sample\tr1\tr2\n")
                for sample in paired_reads:
                    r1 = os.path.join(os.getcwd(), params.OUTPUT_DIR, "".join([sample, "_1.fastq.gz"]))
                    r2 = os.path.join(os.getcwd(), params.OUTPUT_DIR, "".join([sample, "_2.fastq.gz"]))
                    f.write("%s\t%s\t%s\n" % (sample, r1, r2))

        if single_reads:
            with open("samples_single_reads.txt", 'w') as f:
                f.write("sample\tr1\n")
                for sample in single_reads:
                    r1 = os.path.join(os.getcwd(), params.OUTPUT_DIR, "".join([sample, ".fastq.gz"]))
                    f.write("%s\t%s\n" % (sample, r1))
