# -*- coding: utf-8
import os
import anvio
import argparse

from anvio.errors import ConfigError
from anvio.workflows.sra_download import SRADownloadWorkflow

__author__ = "Matthew S. Schechter"
__copyright__ = "Copyright 2017, The anvio Project"
__credits__ = ['mschecht']
__license__ = "GPL 3.0"
__version__ = anvio.__version__
__maintainer__ = "Matthew S. Schechter"
__email__ = "mschechter@uchicago.edu"


"""
This Snakemake workflow downloads and process paired end sequencing data from the Sequence Read Archive (SRA). 

Software requirements: 
- [The SRA toolkit](https://github.com/ncbi/sra-tools): prefetch, fasterq_dump
- [pigz](https://zlib.net/pigz/)

Briefly, the prefetch rule retrieves data from the SRA and stores it as {accession}.sra files. 
Next, fasterq_dump rule extracts the R1 and R2 FASTQ files from the prefetched SRA file. Finally, 
the pigz rule gzips the FASTQ files. 

The input for this workflow is a file of SRA paired end accessions e.g.

$ cat SRA_accession_list.txt
ERR6450080
ERR6450081
ERR6450082

**NOTE**
- This workflow will crash if the SRA accession is not paired-end
"""


M = SRADownloadWorkflow(argparse.Namespace(config=config))
M.init()

dirs_dict = M.dirs_dict

rule SRA_DOWNLOAD_WORKFLOW_target_rule:
    """The rule creates all target files for the Snakefile"""
    input: M.target_files


rule prefetch:
    """Prefetch data from the Sequence Read Archive (SRA).

    Inputs:
        None

    Outputs:
        SRA: File in the SRA_prefetch directory with the name {accession}.sra

    Params:
        SRA_output_dir: Output directory for the prefetched data

    Threads:
        The number of threads to use is specified by the prefetch variable

    NOTES: 
    - This is the first rule of the workflow
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{accession}_prefetch.log")
    input:
    output:
        SRA = temp(os.path.join(dirs_dict['SRA_prefetch'], "{accession}", "{accession}.sra"))
    params:
        SRA_output_dir = os.path.join(dirs_dict['SRA_prefetch']),
        max_size = M.get_param_value_from_config(['prefetch', '--max-size'])
    threads: M.T('prefetch')
    run:
        shell("prefetch {wildcards.accession} --output-directory {params.SRA_output_dir} --max-size {params.max_size} >> {log} 2>&1")


rule fasterq_dump:
    """Use fasterq-dump to extract FASTQ files from an SRA prefetch *.sra

    Inputs:
    - SRA file from the output of the prefetch rule

    Outputs:
    - R1 adn R2 FASTQ files

    Params:
    - SRA_INPUT_DIR: directory containing the SRA file
    - OUTPUT_DIR: directory to write the R1 and R2 FASTQ files

    Threads:
    - Number of threads specified in the M object

    NOTES:
    - This workflow is expecting paired-end reads (notice the hard coded `--split-files`). If this SRA accession
    does not contain paired-end reads the rule will fail. In the future, we can make the workflow modularly handle
    different kinds of sequencing files from SRA with a file like this:

    $ cat SRA_accession_list.txt
    SRA_accession   file_type
    ERR6450080  paired
    ERR6450081  single
    ERR6450082  paired
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{accession}_fasterq_dump.log")
    input: rules.prefetch.output.SRA
    output:
        R1 = os.path.join(dirs_dict['FASTAS'], "{accession}_1.fastq"),
        R2 = os.path.join(dirs_dict['FASTAS'], "{accession}_2.fastq")
    params:
        SRA_INPUT_DIR = os.path.join(dirs_dict['SRA_prefetch'], "{accession}"),
        OUTPUT_DIR = dirs_dict['FASTAS']
    threads: M.T('fasterq_dump')
    run:
        shell("fasterq-dump {params.SRA_INPUT_DIR} -t FASTERQDUMP_TEMP --outdir {params.OUTPUT_DIR} --split-files --verbose --progress --threads {threads} >> {log} 2>&1")

        # Check if fasterq-dump encountered a Disk quota exceeded error
        error_message = "Disk quota exceeded"
        log_path = str(log) 
        with open(log_path, "r") as log_file:
            log_contents = log_file.read()
            if error_message in log_contents:
                raise Exception("fasterq-dump encountered a Disk quota exceeded when processing {wildcards.accession}")


rule pigz:
    """Compress FASTQ files using pigz in parallel!

    Inputs:
    - R1 and R2 FASTQ files from the output of the fasterq_dump rule

    Outputs:
    - R1 and R2 gzipped FASTQ files in the FASTAS directory

    Params:
    - FASTAS: prefix of the FASTQ files in the FASTAS directory

    Threads:
    - Number of threads specified in the M object

    example:
        pigz --processes 8 --verbose 02_FASTA/ERR6450080_1.fastq 02_FASTA/ERR6450080_2.fastq >> 00_LOGS/ERR6450080_fasterq_dump.log 2>&1
    """


    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "{accession}_pigz.log")
    input: 
        expand(os.path.join(dirs_dict['FASTAS'], "{{accession}}_{RUN}.fastq"), RUN=["1", "2"])
    output:
        R1 = os.path.join(dirs_dict['FASTAS'], "{accession}_1.fastq.gz"),
        R2 = os.path.join(dirs_dict['FASTAS'], "{accession}_2.fastq.gz")
    params:
        FASTAS = os.path.join(dirs_dict['FASTAS'], "{accession}")
    threads: M.T('pigz')
    run:
        shell("pigz --processes {threads} --verbose {input} >> {log} 2>&1")