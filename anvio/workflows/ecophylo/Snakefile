# -*- coding: utf-8
import os
import json
import os.path
import argparse

import numpy as np
import pandas as pd

import anvio
import anvio.db as db
import anvio.utils as u
import anvio.tables as t
import anvio.workflows as w

from Bio import SeqIO
from anvio.errors import ConfigError
from anvio.dbops import ContigsDatabase
from anvio.workflows.ecophylo import EcoPhyloWorkflow

__copyright__ = "Copyleft 2015-2024, The Anvi'o Project (http://anvio.org/)"
__credits__ = ['mschecht']
__license__ = "GPL 3.0"
__version__ = anvio.__version__
__maintainer__ = "Matthew S. Schechter"
__email__ = "mschechter@uchicago.edu"


M = EcoPhyloWorkflow(argparse.Namespace(config=config))
M.init()

dirs_dict = M.dirs_dict

rule ECOPHYLO_WORKFLOW_target_rule:
    """The rule creates all target files for the Snakefile"""
    input: M.target_files


def get_hmm_threads(wildcards):
    """This function conditionally selects threads for anvi-run-hmms based on 
    if a contigs-db is a metagenome or not.
    
    Parameters
    ==========
    wildcards: snakemake object
        allows you to programmatically access snakemake wildcards

    Returns
    =======
    threads : int
        number of threads to be used
    """
    threads = M.T('anvi_run_hmms_hmmsearch')
    max_threads = M.get_param_value_from_config("max_threads")
    if not max_threads:
        max_threads = float("Inf")

    if M.metagenomes:
        if wildcards.sample_name in M.metagenomes_name_list:
            threads = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', 'threads_metagenomes'])
    else:
        threads = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', 'threads_genomes'])
    
    if threads:
        try:
            if int(threads) > float(max_threads):
                return int(max_threads)
            else:
                return int(threads)
        except:
            raise ConfigError(f'"threads" must be an integer number. In your config file you provided "{threads}" for '
                              f'the number of threads for rule anvi_run_hmms_hmmsearch')
    else:
        return 1


rule anvi_run_hmms_hmmsearch:
    """Run hmmsearch with input hmms to get domtblout"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_run_hmms_hmmsearch-{sample_name}-{hmm}.log")
    input:
    output:
        done = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{hmm}-dom-hmmsearch", "contigs-hmmsearch.done"),
        hmm_hits = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{hmm}-dom-hmmsearch", "hmm_hits.txt")
    params:
        hmm_source = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', '--installed-hmm-profile']),
        additional_params = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', 'additional_params']),
    threads: get_hmm_threads
    run:
        contigs_db_path = os.path.join(M.contigs_db_name_path_dict[wildcards.sample_name])
        hmm_dir = os.path.join(M.hmm_dict[wildcards.hmm]['path'])
        hmm_source = M.hmm_dict[wildcards.hmm]['source']
        hmmer_output_dir = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.hmm}-dom-hmmsearch")

        if M.metagenomes:
            if wildcards.sample_name in M.metagenomes_name_list:
                threads = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', 'threads_metagenomes']),
                threads = int(threads[0])
        else:
            threads = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', 'threads_genomes']),

        domtblout = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{wildcards.hmm}", "dom-hmmsearch", "hmm.domtable")

        # Run different hmm search depending on whether a hmm is internal or external because anvio
        if hmm_source in M.internal_hmm_sources:
            if not os.path.exists(domtblout):
                print(f"Running internal hmm dataset: {hmm_source}")
                shell("anvi-run-hmms -c {contigs_db_path} \
                                     --hmmer-program hmmsearch \
                                     --hmmer-output-dir {hmmer_output_dir} \
                                     --installed-hmm-profile {hmm_source} \
                                     --domain-hits-table \
                                     --just-do-it \
                                     -T {threads} >> {log} 2>&1")

            # Load contigs_db
            contigs_db = ContigsDatabase(contigs_db_path)
            # Check if anvi-run-scg-taxonomy has been run
            scg_taxonomy_was_run_value = contigs_db.meta['scg_taxonomy_database_version']

            if scg_taxonomy_was_run_value !=  "GTDB: v214.1; Anvi'o: v1":
                print(f"Running anvi-run-scg-taxonomy on {contigs_db_path} since the {wildcards.hmm} is in anvio's collection of SCGs")
                shell("anvi-run-scg-taxonomy -c {contigs_db_path} --num-threads {threads} {params.additional_params}")
        else:
            if not os.path.exists(domtblout):
                print(f"Running external hmm dataset: {wildcards.hmm}")
                shell("anvi-run-hmms -c {contigs_db_path} \
                                     --hmmer-program hmmsearch \
                                     --hmm-profile-dir {hmm_dir} \
                                     --hmmer-output-dir {hmmer_output_dir} \
                                     --domain-hits-table \
                                     --just-do-it \
                                     -T {threads} >> {log} 2>&1")

        # Get hmm_hits.txt
        get_hmm_hits_txt(contigs_db_path, output.hmm_hits)
        
        shell("touch {output.done}")


def get_hmm_hits_txt(contigs_db, out_file):
    """This function extracts the hmm_hits table from a contigs-db
    
    Parameters
    ==========
    contigs_db

    out_file : str
        Output file path for hmm_hits.tsv

    Returns
    =======
    hmm_hits.tsv : tsv 
    """

    database = db.DB(contigs_db, None, ignore_version=True)
    tables_in_database = database.get_table_names()
    args_table = 'hmm_hits'

    if args_table not in tables_in_database:
        # Make empty hmm_hits.txt so that the rule the next rule has something to work with
        column_names = ['entry_id', 'source', 'gene_unique_identifier', 'gene_callers_id', 'gene_name', 'gene_hmm_id', 'e_value']
        df = pd.DataFrame(columns = column_names)
        df.to_csv(out_file, sep="\t", index=False, header=True)
    else:
        table_columns = database.get_table_structure(args_table)
        table_content = database.get_table_as_dataframe(args_table, columns_of_interest=table_columns, error_if_no_data=False)
        u.store_dataframe_as_TAB_delimited_file(table_content, out_file)


rule filter_hmm_hits_by_model_coverage:
    """Remove weak hmm hits using model alignment coverage filter"""

    version: 1.0
    log: 
        log = os.path.join(dirs_dict['LOGS_DIR'], "filter_hmm_hits_by_model_coverage-{sample_name}-{hmm}.log")
    input:
        done = ancient(rules.anvi_run_hmms_hmmsearch.output.done),
        hmm_hits = ancient(rules.anvi_run_hmms_hmmsearch.output.hmm_hits),
    output:
        done = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{hmm}-dom-hmmsearch", "{sample_name}-{hmm}-DB_filtered.done"),
        hmm_hits_filtered = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{hmm}-dom-hmmsearch", "hmm_hits_filtered.txt")
    params:
        model_coverage = M.get_param_value_from_config(['filter_hmm_hits_by_model_coverage', '--min-model-coverage']),
        partial_ORFs = M.get_rule_param('filter_hmm_hits_by_model_coverage', '--filter-out-partial-gene-calls'),
        additional_params = M.get_param_value_from_config(['filter_hmm_hits_by_model_coverage', 'additional_params'])
    threads: M.T('filter_hmm_hits_by_model_coverage')
    run:
        contigs_db = os.path.join(M.contigs_db_name_path_dict[wildcards.sample_name])
        hmm_dir = os.path.join(M.hmm_dict[wildcards.hmm]['path'])
        hmm_source = M.hmm_dict[wildcards.hmm]['source']
        domtblout = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.hmm}-dom-hmmsearch", "hmm.domtable")

        # import hmm_hits to find out how many hmm_hits we got from the hmm model
        # if we don't have any then we can skippp all dis
        df = pd.read_csv(input.hmm_hits, sep='\t')
        df = df[df.source == hmm_source]
        gene_name_list = df['gene_name'].tolist()

        # There is no point in running the following if we don't have the target hmm in the contigs_db!
        if wildcards.hmm in gene_name_list:
            print("we have a hit!")
            if hmm_source in M.internal_hmm_sources:

                shell("anvi-script-filter-hmm-hits-table -c {contigs_db} \
                                                        --domain-hits-table {domtblout} \
                                                        --hmm-source {hmm_source} \
                                                        --min-model-coverage {params.model_coverage} \
                                                        {params.partial_ORFs} \
                                                        {params.additional_params} >> {log} 2>&1")
            else:

                shell("anvi-script-filter-hmm-hits-table -c {contigs_db} \
                                                        --domain-hits-table {domtblout} \
                                                        --hmm-profile-dir {hmm_dir} \
                                                        --hmm-source {hmm_source} \
                                                        --min-model-coverage {params.model_coverage} \
                                                        {params.partial_ORFs} \
                                                        {params.additional_params} >> {log} 2>&1")
            
        else:
            no_hmm_hits_string = f"The hmm {wildcards.hmm} was not found in the hmm_hits of the contigs_db: {contigs_db}"
            print(no_hmm_hits_string)
            with open(log.log, "w") as logfile:
                logfile.write(no_hmm_hits_string)
        
        # Get hmm_hits.txt
        get_hmm_hits_txt(contigs_db, output.hmm_hits_filtered)

        shell('touch {output.done}')


rule process_hmm_hits:
    """Extract AA/NT fastas and external-gene-calls from contigs-db then rename external-gene-calls
    names with reformated names. 
    """

    version: 1.0
    log: 
        log = os.path.join(dirs_dict['LOGS_DIR'], "process_hmm_hits-{sample_name}-{hmm}.log")
    input:
        done = ancient(rules.filter_hmm_hits_by_model_coverage.output.done),
        hmm_hits = ancient(rules.filter_hmm_hits_by_model_coverage.output.hmm_hits_filtered),
    output:
        done = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{hmm}-dom-hmmsearch", "{sample_name}-{hmm}-processed.done"),
    threads: M.T('process_hmm_hits')
    run:
        contigs_db = os.path.join(M.contigs_db_name_path_dict[wildcards.sample_name])
        # hmm_dir = os.path.join(M.hmm_dict[wildcards.hmm]['path'])
        hmm_source = M.hmm_dict[wildcards.hmm]['source']
        # domtblout = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.hmm}-dom-hmmsearch", "hmm.domtable")
        fasta_output_dir = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}")
        if not os.path.exists(fasta_output_dir):
            os.mkdir(fasta_output_dir)
        faa = os.path.join(fasta_output_dir, f"{wildcards.sample_name}-{wildcards.hmm}-hmm_hits.faa")
        fna = os.path.join(fasta_output_dir, f"{wildcards.sample_name}-{wildcards.hmm}-hmm_hits.fna")

        # anvi-script-reformat-fasta output files
        fasta_NT = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.sample_name}-{wildcards.hmm}-hmm_hits_renamed.fna"),
        fasta_AA = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.sample_name}-{wildcards.hmm}-hmm_hits_renamed.faa"),
        report_file_NT = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.sample_name}-{wildcards.hmm}-reformat_report_nt.txt"),
        report_file_AA = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.sample_name}-{wildcards.hmm}-reformat_report_AA.txt")

        # anvi-get-sequences-for-gene-calls input files
        external_gene_calls = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.sample_name}-{wildcards.hmm}-external_gene_calls.tsv")
        fasta = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.sample_name}-{wildcards.hmm}-orfs.fna")

        # rename external gene calls                                                       
        external_gene_calls_renamed = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.sample_name}-{wildcards.hmm}-external_gene_calls_renamed.tsv")

        # import hmm_hits to find out how many hmm_hits we got from the hmm model
        df = pd.read_csv(input.hmm_hits, sep='\t')
        df = df[df.source == hmm_source]
        gene_name_list = df['gene_name'].tolist()

        # There is no point in running the following if we don't have any hmm_hits to begin with!
        if wildcards.hmm in gene_name_list:            
            if hmm_source in M.internal_hmm_sources:

                # Get AA and NT fasta files
                shell("echo -e 'anvi-get-sequences-for-hmm-hits AA stdout:\n' >> {log}")
                shell("echo -e '' >> {log}")
                shell("anvi-get-sequences-for-hmm-hits -c {contigs_db} \
                                                       --hmm-sources  {hmm_source} \
                                                       --gene-names {wildcards.hmm} \
                                                       --get-aa-sequences \
                                                       -o {faa} --just-do-it >> {log} 2>&1")
                shell("echo -e '' >> {log}")
                
                shell("echo -e 'anvi-get-sequences-for-hmm-hits NT stdout:\n' >> {log}")
                shell("echo -e '' >> {log}")
                shell("anvi-get-sequences-for-hmm-hits -c {contigs_db} \
                                                       --hmm-sources  {hmm_source} \
                                                       --gene-names {wildcards.hmm} \
                                                       -o {fna} --just-do-it >> {log} 2>&1")
                shell("echo -e '' >> {log}")


            else:
            
                # Get AA and NT fasta files
                shell("echo -e 'anvi-get-sequences-for-hmm-hits AA stdout:\n' >> {log}")
                shell("anvi-get-sequences-for-hmm-hits -c {contigs_db} \
                                                        --hmm-sources  {hmm_source} \
                                                        --get-aa-sequences \
                                                        -o {faa} --just-do-it >> {log} 2>&1")
                shell("echo -e '' >> {log}")

                shell("echo -e 'anvi-get-sequences-for-hmm-hits NT stdout:\n' >> {log}")
                shell("anvi-get-sequences-for-hmm-hits -c {contigs_db} \
                                                        --hmm-sources  {hmm_source} \
                                                        -o {fna} --just-do-it >> {log} 2>&1")

            # Clean up fasta headers for tree calculation
            prefix_list = [wildcards.sample_name, wildcards.hmm]
            prefix = "_".join(prefix_list) 

            shell("echo -e 'anvi-script-reformat-fasta AA stdout:\n' >> {log}")
            shell("anvi-script-reformat-fasta {faa} \
                                    --simplify-names \
                                    --prefix {prefix} \
                                    --report-file {report_file_AA} \
                                    -o {fasta_AA} >> {log} 2>&1")
            shell("echo -e '' >> {log}")

            shell("echo -e 'anvi-script-reformat-fasta NT stdout:\n' >> {log}")
            shell("anvi-script-reformat-fasta {fna} \
                                    --simplify-names \
                                    --prefix {prefix} \
                                    --report-file {report_file_NT} \
                                    -o {fasta_NT} >> {log} 2>&1")

            shell("echo -e 'anvi-get-sequences-for-gene-calls stdout:\n' >> {log}")
            shell('anvi-get-sequences-for-gene-calls -c {contigs_db} \
                                                     --external-gene-calls {external_gene_calls} \
                                                     -o {fasta} >> {log} 2>&1')

            
            # Import tables
            external_gene_calls = pd.read_csv(external_gene_calls, delim_whitespace=True, index_col=False)

            reformat_file = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}", f"{wildcards.sample_name}-{wildcards.hmm}-reformat_report_AA.txt")
            reformat_report = pd.read_csv(reformat_file, sep="\t", index_col=False, names=["new_header", "header"])

            # Parse input files
            #-----------------
            # Parse reformat_report to get gene-callers-ids
            reformat_report["gene_callers_id"] = reformat_report['header'].str.split("gene_callers_id:|\|start:", expand=True)[1].astype(str)

            # Parse external-gene-calls contig column to get gene-callers-ids
            external_gene_calls[['name', 'contig_number', 'gene_callers_id']] = external_gene_calls['contig'].str.rsplit('_',2, expand=True)

            # Join external-gene-calls with reformat-report on gene-callers-id
            #-----------------
            external_gene_calls = external_gene_calls.merge(reformat_report, on="gene_callers_id", how="inner")

            # Replace contig name from anvi-export-gene-calls with the simplified, new header from anvi-script-reformat-fasta
            external_gene_calls = external_gene_calls[["gene_callers_id", "new_header", "start", "stop", "direction", "partial", "call_type", "source", "version", "aa_sequence"]]
            external_gene_calls = external_gene_calls.rename(columns={'new_header': 'contig'})

            # Write file
            external_gene_calls.to_csv(external_gene_calls_renamed, sep="\t", index=False, header=True)

            shell('touch {output.done}')
        else:
            shell('touch {output.done}')


rule combine_sequence_data:
    """Cat all NT/AA fastas, reformat files, and renamed external-gene-calls files 
    Narrow end of the workflow funnel! 

    In this rule output files from metagenomes, genomes, SAGs, and MAGs all come together here!

    cat all neccessary files:
        - AA sequences
        - NT sequences
        - header reformat files
        - external-gene-calls sequences and reformat_files from seperate metagenomes, genomes, SAGs, or MAGs into one fasta
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "combine_sequence_data_{hmm}.log")
    input:
        done_files = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{{hmm}}-dom-hmmsearch", "{sample_names}-{{hmm}}-processed.done"), sample_names = M.names_list),
        hmm_hits = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{{hmm}}-dom-hmmsearch", "hmm_hits_filtered.txt"), sample_names = M.names_list),
    output:
        NT_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-all.fna"),
        AA_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-all.faa"),
        reformat_report_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-reformat-report-all.txt"),
        external_gene_calls_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-external_gene_calls_all.tsv"),
        done = touch(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-combine_sequence_data.done"))
    threads: M.T('combine_sequence_data')
    run:
        # Make list of contigDBs that had a hit with the hmm
        hmm_source = M.hmm_dict[wildcards.hmm]['source']

        contigs_db_with_hmm_hits = []
        contigs_db_with_hmm_NO_hits = []
        for hmm_hits in input.hmm_hits:
            df = pd.read_csv(hmm_hits, sep='\t')
            df = df[df.source == hmm_source]
            gene_name_list = df['gene_name'].tolist()
            if wildcards.hmm in gene_name_list:
                contigs_db = hmm_hits.split("/")[2].split("-")[0]
                contigs_db_with_hmm_hits.append(contigs_db)
            else:
                contigs_db = hmm_hits.split("/")[2].split("-")[0]
                contigs_db_with_hmm_NO_hits.append(contigs_db)
        
        outfile = os.path.join(dirs_dict['LOGS_DIR'], f"contigDBs_with_no_hmm_hit_{wildcards.hmm}.log") 
        with open(outfile, "a") as outfile:
            for element in contigs_db_with_hmm_NO_hits:
                outfile.write(element + '\n')
        
        NT_list = []
        AA_list = []
        AA_reformat_list = []
        external_gene_calls_reformat_list = []

        for contigs_db in contigs_db_with_hmm_hits:
           NT_path = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{contigs_db}", f"{contigs_db}-{wildcards.hmm}-hmm_hits_renamed.fna") 
           AA_path = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{contigs_db}", f"{contigs_db}-{wildcards.hmm}-hmm_hits_renamed.faa") 
           NT_reformat_path = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{contigs_db}", f"{contigs_db}-{wildcards.hmm}-reformat_report_NT.txt") 
           AA_reformat_path = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{contigs_db}", f"{contigs_db}-{wildcards.hmm}-reformat_report_AA.txt") 
           external_gene_calls_reformat_reformat_path = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{contigs_db}", f"{contigs_db}-{wildcards.hmm}-external_gene_calls_renamed.tsv") 
           NT_list.append(NT_path)
           AA_list.append(AA_path)
           AA_reformat_list.append(AA_reformat_path)
           external_gene_calls_reformat_list.append(external_gene_calls_reformat_reformat_path)

        with open(output.NT_all, "a") as NT_output:
            for f in NT_list:
                NT_output.write(open(f).read())
        with open(output.AA_all, "a") as AA_all:
            for f in AA_list:
                AA_all.write(open(f).read())
        with open(output.reformat_report_all, "a") as reformat_report_all:
            for f in AA_reformat_list:
                reformat_report_all.write(open(f).read())

        col_names = ["gene_callers_id", "contig", "start", "stop", "direction", "partial", "call_type", "source", "version", "aa_sequence"]
        with open(output.external_gene_calls_all, "a") as external_gene_calls_all:
                external_gene_calls_all.write('\t'.join(col_names) + "\n")
                for f in external_gene_calls_reformat_list:
                    file = open(f).read().splitlines(True)
                    external_gene_calls_all.writelines(file[1:])


rule cluster_X_percent_sim_mmseqs:
    """Cluster NT or AA fasta file with user defined percent identity"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cluster_X_mmseqs_{hmm}.log")
    input: 
        done = rules.combine_sequence_data.output.done
    output: 
        fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-mmseqs_NR_rep_seq.fasta"),
        mmseqs_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-mmseqs_NR_cluster.tsv"),
        done = touch(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-mmseqs_NR_cluster.done"))
    params:
        output_prefix = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-mmseqs_NR"),
        mmseqs_tmp = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-tmp"),
        min_seq_id = M.get_param_value_from_config(['cluster_X_percent_sim_mmseqs', '--min-seq-id']),
        cov_mode = M.get_param_value_from_config(['cluster_X_percent_sim_mmseqs', '--cov-mode']),
        additional_params = M.get_param_value_from_config(['cluster_X_percent_sim_mmseqs', 'additional_params'])
    threads: M.T('cluster_X_percent_sim_mmseqs')
    run:
        if M.AA_mode == True:
           fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], f"{wildcards.hmm}", f"{wildcards.hmm}-all.faa") 
        else:
           fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], f"{wildcards.hmm}", f"{wildcards.hmm}-all.fna")

        # Exit workflow if we couldn't find any hmm-hits
        from Bio import SeqIO

        fasta_dict = SeqIO.index(fasta, "fasta")
        if len(fasta_dict) == 0:
            raise ConfigError(f"anvi'o and the EcoPhylo workflow are sad to announce that the "
                              f"hmm, {wildcards.hmm}, was not not found in any of your contigs_dbs")
            
        shell(f"mmseqs easy-cluster {fasta} \
                                    {params.output_prefix} \
                                    {params.mmseqs_tmp} \
                                    --threads {threads} \
                                    --min-seq-id {params.min_seq_id} \
                                    --cov-mode {params.cov_mode} \
                                    {params.additional_params} >> {log} 2>&1")


rule cluster_X_percent_sim_mmseqs_OTUs:
    """Cluster extracted proteins within percent identity parameter space provided by user. 
    This will help identify clustering thresholds for OTU like analyses.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cluster_X_mmseqs_{hmm}_{clustering_threshold}.log")
    input: 
        NT_all = rules.combine_sequence_data.output.NT_all,
        done = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-mmseqs_NR_cluster.done")
    output: 
        fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{clustering_threshold}", "{hmm}-{clustering_threshold}-mmseqs_NR_rep_seq.fasta"),
        mmseqs_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{clustering_threshold}", "{hmm}-{clustering_threshold}-mmseqs_NR_cluster.tsv")
    params:
        output_prefix = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{clustering_threshold}", "{hmm}-{clustering_threshold}-mmseqs_NR"),
        mmseqs_tmp = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{clustering_threshold}", "{hmm}-{clustering_threshold}-tmp"),
        cov_mode = M.get_param_value_from_config(['cluster_X_percent_sim_mmseqs', '--cov-mode']),
        additional_params = M.get_param_value_from_config(['cluster_X_percent_sim_mmseqs', 'additional_params'])
    threads: M.T('cluster_X_percent_sim_mmseqs')
    run:
        min_seq_id = M.clustering_threshold_dict[wildcards.clustering_threshold]
        shell("mmseqs easy-cluster {input.NT_all} \
                                   {params.output_prefix} \
                                   {params.mmseqs_tmp} \
                                   --threads {threads} \
                                   --min-seq-id {min_seq_id} \
                                   --cov-mode {params.cov_mode} \
                                   {params.additional_params} >> {log} 2>&1")


if M.cluster_representative_method == "cluster_rep_with_coverages":
    rule anvi_profile_blitz:
        """Choose a NT cluster rep based on read recruitment!
        The sequence with the most read recruitment from the input profiled assembly will be chosen as the cluster representative.
        """

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_profile_blitz-{sample_name}.log")
        input:
        output:
            os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-gene-coverages.txt"),
        params:
        threads: M.T('anvi_profile_blitz')
        run:
            contigs_db = os.path.join(M.contigs_db_name_path_dict[wildcards.sample_name])
            bam = os.path.join(M.contigs_db_name_bam_dict[wildcards.sample_name])

            # Run different hmm search depending on whether a hmm is internal or external because anvio
            shell("anvi-profile-blitz {bam} -c {contigs_db} --gene-mode --report-minimal -o {output} >> {log} 2>&1")


    rule cat_anvi_profile_blitz:
        """Cat gene coverages from anvi-profile-blitz"""

        version: 1.0
        # log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_profile_blitz-{sample_name}.log")
        input:
            expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-gene-coverages.txt"), sample_name = M.names_list),
        output:
            os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "gene-coverages.txt"),
        params:
        threads: M.T('anvi_profile_blitz')
        run:
            shell("echo -e 'gene_callers_id\tcontig\tsample\tlength\tdetection\tmean_cov' > {output}")
            shell("awk 'FNR>1' {input} >> {output}")


    rule pick_cluster_rep_with_coverage:
        """Pick a cluster rep with coverage values."""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "pick_cluster_rep_with_coverage-{hmm}.log")
        input:
            mmseqs_cluster_rep_index = rules.cluster_X_percent_sim_mmseqs.output.mmseqs_cluster_rep_index,
            reformat_report = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-reformat-report-all.txt"),
            coverages = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "gene-coverages.txt") 
        output:
            coverage_reps = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-coverage-headers.txt"),
            coverage_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-coverage_cluster.tsv")
        params:
        run:
            # Bind anvi-profile-blitz data with cluster rep data and group_by cluster rep then find the cluster member with the highest coverage to pick new rep
            pd.set_option('expand_frame_repr', False)

            cluster_rep_index = pd.read_csv(input.mmseqs_cluster_rep_index, \
                                    sep="\t", \
                                    index_col=False, \
                                    names=["representative", "cluster_members"])

            reformat_report = pd.read_csv(input.reformat_report, \
                                            sep="\t", \
                                            index_col=False, \
                                            names=["new_header", "header"])

            bam = pd.read_csv(input.coverages, sep="\t", index_col=False)

            bam['primary_key'] = bam['contig'] + "_" + bam['gene_callers_id'].astype(str)
            reformat_report["gene_callers_id"] = reformat_report['header'].str.split("gene_callers_id:|\|start:", expand=True)[1].astype(str)
            reformat_report["contig"] = reformat_report['header'].str.split("contig:|\|gene_callers_id:", expand=True)[1].astype(str)
            reformat_report['primary_key'] = reformat_report['contig'] + "_" + reformat_report['gene_callers_id'].astype(str)
            
            df = pd.merge(reformat_report, bam, on='primary_key', how='inner')
            df2 = pd.merge(cluster_rep_index, df, left_on='cluster_members', right_on='new_header', how='inner')[["representative", "cluster_members", "mean_cov"]]

            def get_new_seed(df):
                """This function extracts the hmm_hits table from a contigs-db
                
                Parameters
                ==========
                contigs_db

                out_file : str
                    Output file path for hmm_hits.tsv

                Returns
                =======
                hmm_hits.tsv : tsv 
                """
                new_seed = df[df['mean_cov'] == df['mean_cov'].max()]['cluster_members'].iloc[0]
                df['representative'] = [new_seed] * len(df)
                return df

            df3 = df2.groupby('representative').apply(get_new_seed)

            # export headers of representatives and cluster rep index
            df3[["representative"]].drop_duplicates().to_csv(output.coverage_reps, sep="\t", index=None, header=False)
            df3[["representative", "cluster_members"]].to_csv(output.coverage_cluster_rep_index, sep="\t", index=None, na_rep="NA")



    rule subset_AA_seqs_with_coverage_reps:
        """Subset AA sequences for the mmseqs cluster representatives"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "subset_AA_seqs_with_coverage_reps_{hmm}.log")
        input:
            fa = rules.combine_sequence_data.output.AA_all,
            coverage_reps = rules.pick_cluster_rep_with_coverage.output.coverage_reps,
        output:
            fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-AA_subset.fa"),
        threads: M.T('subset_AA_seqs_with_coverage_reps')
        run:
            shell("anvi-script-reformat-fasta {input.fa} -I {input.coverage_reps} -o {output.fasta} >> {log} 2>&1")


if M.cluster_representative_method == "mmseqs":
    rule subset_AA_seqs_with_mmseqs_reps:
        """Subset AA sequences for the mmseqs cluster representatives"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "subset_AA_seqs_with_mmseqs_reps_{hmm}.log")
        input:
            fa = rules.combine_sequence_data.output.AA_all,
            mmseqs_reps = rules.cluster_X_percent_sim_mmseqs.output.fasta,
        output:
            fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-AA_subset.fa"),
        params:
            headers = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-headers.tmp")
        threads: M.T('subset_AA_seqs_with_mmseqs_reps')
        run:
            shell("grep '>' {input.mmseqs_reps} | sed 's/>//g' > {params.headers}")
            shell("anvi-script-reformat-fasta {input.fa} -I {params.headers} -o {output.fasta} >> {log} 2>&1")


rule align_sequences:
    """MSA of AA sequences subset."""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "align_sequences_{hmm}.log")
    input: os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-AA_subset.fa"),
    output: os.path.join(dirs_dict['MSA'], "{hmm}", "{hmm}-aligned.fa"),
    params: additional_params = M.get_param_value_from_config(['align_sequences', 'additional_params'])
    threads: M.T('align_sequences')
    run:
        shell("muscle -in {input} -out {output} {params.additional_params} -verbose 2> {log}")

rule trim_alignment:
    """Trim MSA alignment"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "trim_alignment_{hmm}.log")
    input: rules.align_sequences.output
    output: os.path.join(dirs_dict['MSA'], "{hmm}", "{hmm}_aligned_trimmed.fa")
    params:
        gt = M.get_param_value_from_config(['trim_alignment', '-gt']),
        gappyout = M.get_rule_param('trim_alignment', '-gappyout'),
        additional_params = M.get_param_value_from_config(['trim_alignment', 'additional_params'])
    threads: M.T('trim_alignment')
    run:
        shell('trimal -in {input} -out {output} {params.gappyout} {params.additional_params} 2> {log}')


rule remove_sequences_with_X_percent_gaps:
    """Remove sequences that have X% gaps"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "remove_sequences_with_X_percent_gaps_{hmm}.log")
    input: rules.trim_alignment.output
    output: 
        fasta = os.path.join(dirs_dict['MSA'], "{hmm}", "{hmm}_aligned_trimmed_filtered.fa")
    params:
        seq_counts_tsv = os.path.join(dirs_dict['MSA'], "{hmm}", "{hmm}_gaps_counts"),
        max_percentage_gaps = M.get_param_value_from_config(['remove_sequences_with_X_percent_gaps', '--max-percentage-gaps'])
    threads: M.T('remove_sequences_with_X_percent_gaps')
	run:
		shell("anvi-script-reformat-fasta {input} -o {output.fasta} \
									              --max-percentage-gaps {params.max_percentage_gaps} \
									              --export-gap-counts-table {params.seq_counts_tsv} >> {log} 2>&1")


rule count_num_sequences_filtered:
    """Record the number of sequences filtered at each step of the workflow"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "count_num_sequences_filtered_{hmm}.log")
    input:
        step1 = rules.combine_sequence_data.output.NT_all,
        step2 = rules.cluster_X_percent_sim_mmseqs.output.fasta,
        step3 = rules.remove_sequences_with_X_percent_gaps.output.fasta,
        clustering_thresholds = expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{{hmm}}", "{clustering_threshold}", "{{hmm}}-{clustering_threshold}-mmseqs_NR_rep_seq.fasta"), clustering_threshold = M.clustering_param_space_list_strings),
    output: os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_MSA_STATS'], "{hmm}", "{hmm}_stats.tsv")
    threads: M.T('count_num_sequences_filtered')
    run:
        def count_num_sequences(fasta):
            """This function counts the number of sequences in a fasta file
            
            Parameters
            ==========
            fasta: fasta

            Returns
            =======
            num_seqs : int
            """

            num_seqs = 0
            for line in fasta:
                if line.startswith(">"):
                    num_seqs += 1
            return num_seqs

        input_files_list = [input.step1, input.step2, input.step3]
        num_seqs_list = [count_num_sequences(open(fasta)) for fasta in input_files_list]

        clustering_threshold_attributes_list = []
        for file in input.clustering_thresholds:
            path = file
            threshold = file.split("/")[3]
            with open(file) as fasta:
                num_seqs = count_num_sequences(fasta)
            clustering_threshold_attributes = [str(threshold), str(num_seqs), path]
            clustering_threshold_attributes_list.append(clustering_threshold_attributes)

        with open(output[0], 'w') as f:
            col_names = ["rule_name", "num_sequences_left", "rel_path"]
            step1 = ["combine_sequence_data", str(num_seqs_list[0]), input.step1]
            step2 = ["cluster_X_percent_sim_mmseqs", str(num_seqs_list[1]), input.step2]
            step3 = ["remove_sequences_with_X_percent_gaps", str(num_seqs_list[2]), input.step3]
            lines = [col_names, step1, step2, step3] + clustering_threshold_attributes_list 
            for line in lines:
                f.write('\t'.join(line) + '\n')


rule subset_DNA_reps_with_QCd_AA_reps_for_mapping:
    """Extract NT sequences based on final list of AA sequences for read recruitment"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "subset_DNA_reps_with_QCd_AA_reps_for_mapping_{hmm}.log")
    input:
        fasta = rules.combine_sequence_data.output.NT_all,
        reps = rules.remove_sequences_with_X_percent_gaps.output
    output:
        NT_for_mapping = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-references_for_mapping_NT.fa"),
        headers = os.path.join(dirs_dict['MSA'], "{hmm}", "{hmm}_headers.tmp")
    threads: M.T('subset_DNA_reps_with_QCd_AA_reps_for_mapping')
    run:
        shell("grep '>' {input.reps} | sed 's/>//g' > {output.headers}")

        shell("anvi-script-reformat-fasta {input.fasta} -I {output.headers} -o {output.NT_for_mapping} >> {log} 2>&1")


rule subset_external_gene_calls_file_all:
    """Subset the concatenated external_gene_calls.txt for the final set of NT sequences for profiling.
    ALSO, once all external-gene-calls are concatenated, gene-callers-ids will be re-indexed so that 
    there are NO repeating gene-callers-id values. 
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "subset_external_gene_calls_file_all_{hmm}.log")
    input:
        external_gene_calls_all = rules.combine_sequence_data.output.external_gene_calls_all,
        headers = os.path.join(dirs_dict['MSA'], "{hmm}", "{hmm}_headers.tmp")
    output:
        external_gene_calls_subset = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-external_gene_calls_subset.tsv"),
    threads: M.T('subset_external_gene_calls_file_all')
    script:
        "scripts/subset_external_gene_calls_file.py"


rule make_fasta_txt:
    """Format a fasta.txt with the filtered NT sequences for profiling in metagenomics workflow"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "make_fasta_txt.log")
    input:
        expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-external_gene_calls_subset.tsv"), hmm = M.hmm_dict.keys()),
        expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-references_for_mapping_NT.fa"), hmm = M.hmm_dict.keys()),
    output:
        fasta_txt = os.path.join(dirs_dict['HOME'], "METAGENOMICS_WORKFLOW", "fasta.txt"),
    threads: M.T('make_fasta_txt')
    run:
        fastas = [os.path.join("..", "02_NR_FASTAS", r + "/" + r + '-references_for_mapping_NT.fa') for r in M.hmm_dict.keys()]
        external_gene_calls = [os.path.join("..", "02_NR_FASTAS", r + "/" + r + '-external_gene_calls_subset.tsv') for r in M.hmm_dict.keys()]

        list_of_strings = ['\t'.join(t) + '\n' for t in zip(M.hmm_dict.keys(), fastas, external_gene_calls)]

        shell('echo -e "name\tpath\texternal_gene_calls" > {output.fasta_txt}')
        shell('echo -n "%s" >> {output.fasta_txt}' % ''.join(list_of_strings))

if M.run_iqtree == True:
  rule iqtree:
      """Calculate a phylogenetic tree using iqtree"""

      version: 1.0
      log: os.path.join(dirs_dict['LOGS_DIR'], "iqtree_{hmm}.log")
      input: rules.remove_sequences_with_X_percent_gaps.output
      output: 
          tree = os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}.iqtree"),
          done = touch(os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}-tree.done"))
      params:
          outfile=os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}"),
          model = M.get_param_value_from_config(['iqtree', '-m']),
          additional_params = M.get_param_value_from_config(['iqtree', 'additional_params'])
      threads: M.T('iqtree')
      run:
          shell('iqtree -s {input} -nt AUTO -m {params.model} -pre {params.outfile}  -T AUTO {params.additional_params} >> {log} 2>&1')

elif M.run_fasttree == True:
  rule fasttree:
      """Want to go faster?? Then Fasttree"""

      version: 1.0
      log: os.path.join(dirs_dict['LOGS_DIR'], "fasttree_{hmm}.log")
      input: rules.remove_sequences_with_X_percent_gaps.output
      output:
          tree = os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}.nwk"),
          done = touch(os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}-tree.done"))
      params:
          additional_params = M.get_param_value_from_config(['fasttree', 'additional_params']),
          tree = os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}.nwk"),
      threads: M.T('fasttree')
      run:
        shell("FastTree -fastest {input} 2> {log} 1> {params.tree}")


rule rename_tree_tips:
    """Add "_split_00001" suffix to tree tips names to bind with profileDB"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "rename_tree_tips_{hmm}.log")
    input:
        tree = os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}-tree.done"),
    output:
        done = os.path.join(dirs_dict['TREES'], "{hmm}_combined.done"),
        tree = os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}_renamed.nwk"),
        fasta = os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}_renamed.faa"),
        fasta_all = os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}_renamed_all.faa")
    params:
        fasttree = os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}.nwk"),
        iqtree = os.path.join(dirs_dict['TREES'], "{hmm}", "{hmm}.iqtree"),
        fasta = os.path.join(dirs_dict['MSA'], "{hmm}", "{hmm}_aligned_trimmed_filtered.fa"),
        fasta_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-all.faa")
    threads: M.T('rename_tree_tips')
    run:
        # rename tree tips 
        from ete3 import Tree

        def add_split_name_to_tree_tips(tree, outfile):
            """This function adds a string to the end of tree tip names in a newick file
            
            Parameters
            ==========
            tree: newick file

            outfile: str
                path to returned newick tree
            """

            t = Tree(tree)
            for leaf in t:
                leaf.name = leaf.name + "_split_00001"

            t.write(format = 1, outfile = outfile)

        if M.run_iqtree == True:
            add_split_name_to_tree_tips(tree = params.iqtree, outfile = output.tree)
        
        elif M.run_fasttree == True:
            add_split_name_to_tree_tips(tree = params.fasttree, outfile = output.tree)
        
        # rename fasta headers to match tree

        def add_split_name_to_fasta_headers(fasta, outfile):
            """This function adds a string "_split_00001\n" to the end of fasta file headers
            
            Parameters
            ==========
            fasta: str
                path to fasta file

            outfile: str
                path returned fasta
            """
            fasta = open(str(fasta))
            newfasta = open(outfile, "a")

            for line in fasta:
                if line.startswith('>'):
                    newname = line.rstrip("\n") + "_split_00001\n"
                    newfasta.write(newname)
                else:
                    newfasta.write(line)

            fasta.close()
            newfasta.close()

        add_split_name_to_fasta_headers(params.fasta, output.fasta)
        add_split_name_to_fasta_headers(params.fasta_all, output.fasta_all)

        shell('touch {output.done}')


def extract_misc_data(mmseqs_cluster_rep_index, final_sequences_headers, output):
    """This function creates a tsv of metadata for mmseqs cluster tsv outfile (*_cluster.tsv)
    
    Parameters
    ==========
    mmseqs_cluster_rep_index: str
        path to tsv containing mmseqs cluster representatives and member index

    final_sequences_headers: str
        path to tsv with fasta file headers
    
    output: str
        path to output tsv, metadata values include ['split_name', 'contigs_db_type', 'genomic_seq_in_cluster', 'cluster_size']
        - 'split_name': ID name in interactive interface
        - 'contigs_db_type': cluster representative origin: MAG, SAG, isolate genome, metagenome.
        - 'genomic_seq_in_cluster': detects if a cluster member came from an external-genome
        - 'cluster_size': number of sequences in cluster
    """

    # Import data
    #------------
    cluster_rep_index = pd.read_csv(mmseqs_cluster_rep_index, sep="\t", index_col=False, names=["representative", "cluster_members"])

    final_sequences_headers = pd.read_csv(final_sequences_headers, sep="\t", index_col=False, names=["identifier"])
    
    # Clean data
    #------------
    
    # Detect if there is a genomic reference protein in cluster
    cluster_rep_index_dict = cluster_rep_index.groupby('representative')['cluster_members'].apply(list).to_dict()

    cluster_reps_with_genomic_references_list = []
    for seq in final_sequences_headers.iloc[:, 0].tolist():
        cluster_members_list = cluster_rep_index_dict[seq]
        for external_genome in M.external_genomes_names_list:
            check = any(external_genome in s for s in cluster_members_list)
            if check is True:
                cluster_reps_with_genomic_references_list.append(seq)

    # Count size of clusters
    df = cluster_rep_index.groupby('representative').apply(count_cluster_size)[['cluster_members', 'cluster_size']]

    # Make split names for anvi-interactive
    df['split_name'] = df['cluster_members'].astype(str) + '_split_00001' 

    # subset misc data to final set of proteins
    df = pd.merge(df, final_sequences_headers, left_on='cluster_members', right_on='identifier', how='inner')

    df['genomic_seq_in_cluster'] = np.where(df['cluster_members'].isin(cluster_reps_with_genomic_references_list), 'yes', 'no')

    # Determine contigs_db type: metagenome or genomes
    # FIXME: This will need to be changed in the future to accomidate SAGs, MAGs, and other genomic sources
    # If metagenome_name is in external_genomes_names_list then it's a genome
    contigs_db_type_dict = {}
    for name in list(df.cluster_members):
        if any(x in name for x in M.external_genomes_names_list):
            contigs_db_type_dict[name] = "genome"
        else:
            contigs_db_type_dict[name] = "metagenome"
        
    df["contigs_db_type"] = df.cluster_members.apply(lambda name: contigs_db_type_dict[name])

    # grab the final columns
    df = df[['split_name', 'contigs_db_type', 'genomic_seq_in_cluster', 'cluster_size']]

    # Export
    #-------
    df.to_csv(output, sep="\t", index=None, na_rep="NA")

def count_cluster_size(group):
    c = group['cluster_members'].count()
    group['cluster_size'] = c

    return group


rule make_misc_data:
    """Make misc data file for clustered sequences"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "add_contigs_db_type_{hmm}.log")
    input:
        final_list_of_sequences_for_mapping_headers = os.path.join(dirs_dict['MSA'], "{hmm}", "{hmm}_headers.tmp")
    output:
        misc_data_final = os.path.join(dirs_dict['MISC_DATA'], "{hmm}_misc.tsv")
    params:
        mmseqs_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-mmseqs_NR_cluster.tsv"),
        coverage_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{hmm}", "{hmm}-coverage_cluster.tsv")
    threads: M.T('add_misc_data_to_taxonomy')
    run:
        """Here we determine the origin of each SCG (which kind of contigs_db): metagenome, isolate genome, etc."""
        if M.cluster_representative_method == "mmseqs":
            extract_misc_data(mmseqs_cluster_rep_index = params.mmseqs_cluster_rep_index,
                              final_sequences_headers = input.final_list_of_sequences_for_mapping_headers,
                              output = output.misc_data_final)
        if M.cluster_representative_method == "cluster_rep_with_coverages":
            extract_misc_data(mmseqs_cluster_rep_index = params.coverage_cluster_rep_index,
                              final_sequences_headers = input.final_list_of_sequences_for_mapping_headers,
                              output = output.misc_data_final)



rule anvi_scg_taxonomy:
    """Run anvi-estimate-SCG-taxonomy and import the resulting taxonomy misc data to profileDB for internal hmms only"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_scg_taxonomy_{hmm}.log")
    input:
        final_list_of_sequences_for_mapping_headers = os.path.join(dirs_dict['MSA'], "{hmm}", "{hmm}_headers.tmp"),
        run_scg_taxonomy = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}_scg_taxonomy.done"), sample_name = M.names_list),
    params:
        reformat_file = rules.combine_sequence_data.output.reformat_report_all,
        misc_data_dir = dirs_dict['MISC_DATA'],
        taxonomy = os.path.join(dirs_dict['MISC_DATA'], "{hmm}_estimate_scg_taxonomy_results"),
        taxonomy_long = os.path.join(dirs_dict['MISC_DATA'], "{hmm}_estimate_scg_taxonomy_results-RAW-LONG-FORMAT.txt"),
        tax_data_final = os.path.join(dirs_dict['MISC_DATA'], "{hmm}_scg_taxonomy_data.tsv"),
    output: 
        done = touch(os.path.join(dirs_dict['HOME'], "{hmm}_anvi_estimate_scg_taxonomy_for_SCGs.done"))
    threads: M.T('anvi_scg_taxonomy')
    run:
        hmm_source = M.hmm_dict[wildcards.hmm]['source']

        # Concatenate metagenomes.txt and external-genomes.txt
        contigs_db_name_path_list = list(M.contigs_db_name_path_dict.items())
        contigs_db_name_path_df = pd.DataFrame(contigs_db_name_path_list, columns=['name', 'contigs_db_path'])
        combined_genomes_df_path = f"{wildcards.hmm}_combined_genomes.txt"
        contigs_db_name_path_df.to_csv(combined_genomes_df_path, sep="\t", index=False, header=True)

        shell("mkdir -p {params.misc_data_dir}; \
        anvi-estimate-scg-taxonomy -M {combined_genomes_df_path} \
                                   --metagenome-mode \
                                   --scg-name-for-metagenome-mode {wildcards.hmm} \
                                   -T {threads} \
                                   --raw-output \
                                   -O {params.taxonomy} &> {log}")
        
        # Import data
        #------------
        scg_taxonomy = pd.read_csv(params.taxonomy_long, \
                                   sep="\t", \
                                   index_col=False)
                                        
        reformat_report = pd.read_csv(params.reformat_file, \
                                      sep="\t", \
                                      index_col=False, \
                                      names=["new_header", "header"])

        final_sequences_headers = pd.read_csv(input.final_list_of_sequences_for_mapping_headers, \
                                            sep="\t", \
                                            index_col=False, \
                                            names=["identifier"])
        
        # Clean Data
        #-----------
        reformat_report = pd.merge(reformat_report, final_sequences_headers, left_on='new_header', right_on='identifier', how='inner')
        reformat_report["gene_callers_id"] = reformat_report['header'].str.split("gene_callers_id:|\|start:", expand=True)[1].astype(str)
        reformat_report["new_header_tmp"] = reformat_report['new_header'].str.rsplit('_', 1).str[0]
        reformat_report["identifier"] = reformat_report["new_header_tmp"] + "_" + reformat_report["gene_callers_id"] 
        scg_taxonomy["identifier"] =  scg_taxonomy["metagenome_name"] + "_" +  scg_taxonomy["gene_name"] + "_" +  scg_taxonomy["gene_callers_id"].astype(str)
        scg_taxonomy = scg_taxonomy.merge(reformat_report, on='identifier', how="inner")
        scg_taxonomy['split_name'] = scg_taxonomy['new_header'].astype(str) + '_split_00001' 
        scg_taxonomy = scg_taxonomy[["split_name", "identifier", "percent_identity", "t_domain", "t_phylum", "t_class", "t_order", "t_family", "t_genus", "t_species"]]

        # Export
        #-------
        scg_taxonomy.to_csv(params.tax_data_final, \
                            sep="\t", \
                            index=None, \
                            na_rep="NA")


if M.samples_txt_file:
    # PROFILE-MODE with read recruitment
    include: "rules/profile_mode.smk"
else:
    # TREE-MODE
    include: "rules/tree_mode.smk"
