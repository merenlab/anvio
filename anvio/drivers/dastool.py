# -*- coding: utf-8
# pylint: disable=line-too-long
"""
"""
import os

import anvio
import anvio.utils as utils
import anvio.terminal as terminal
import anvio.ccollections as ccollections

from anvio.errors import ConfigError


__author__ = "Developers of anvi'o (see AUTHORS.txt)"
__copyright__ = "Copyleft 2015-2019, the Meren Lab (http://merenlab.org/)"
__credits__ = []
__license__ = "GPL 3.0"
__version__ = anvio.__version__
__maintainer__ = "Ã–zcan Esen"
__email__ = "ozcanesen@gmail.com"


run = terminal.Run()
progress = terminal.Progress()


class DAS_Tool:
    arguments = {
        'source-collections': (
                ['-S', '--source-collections'],
                {'metavar': "COLLECTION_LIST",
                 'required': True,
                 'help': "Comma-separated list of collections, case sensitive."}
                    ),
        'search-engine': (
                ['--search-engine'],
                {'metavar': "PROGRAM",
                 'required': False,
                 'default': 'usearch',
                 'help': "Engine used for single copy gene identification [blast/diamond/usearch].\
                              (default: usearch)"}
                    ),
        'score-threshold': (
                ['--score-threshold'],
                {'metavar': "FLOAT",
                 'required': False,
                 'help': "Score threshold until selection algorithm will keep selecting bins [0..1].\
                              (default: 0.5)"}
                    ),
        'duplicate-penalty': (
                ['--duplicate-penalty'],
                {'metavar': "FLOAT",
                 'required': False,
                 'help': "Penalty for duplicate single copy genes per bin (weight b).\
                              Only change if you know what you're doing. [0..3]\
                              (default: 0.6)"}
                    ),
        'megabin-penalty': (
                ['--megabin-penalty'],
                {'metavar': "FLOAT",
                 'required': False,
                 'help': "Penalty for megabins (weight c). Only change if you know what you're doing. [0..3]\
                              (default: 0.5)"}
                    ),
        'db-directory': (
                ['--db-directory'],
                {'metavar': "PATH",
                 'required': False,
                 'default': None,
                 'help': "Directory of single copy gene database. (default: install_dir/db)"}
                    ),
    }

    citation = "Christian M. K. Sieber, Alexander J. Probst, Allison Sharrar, Brian C. Thomas, \
                Matthias Hess, Susannah G. Tringe & Jillian F. Banfield (2018). \
                Recovery of genomes from metagenomes via a dereplication, aggregation \
                and scoring strategy. Nature Microbiology. https://doi.org/10.1038/s41564-018-0171-1."

    cluster_type = 'split'


    def __init__(self, run=run, progress=progress):
        self.run = run
        self.progress = progress

        self.program_name = 'DAS_Tool'

        utils.is_program_exists(self.program_name)


    def cluster(self, input_files, args, work_dir, threads=1):
        J = lambda p: os.path.join(work_dir, p)

        cwd_backup = os.getcwd()
        os.chdir(work_dir)
        log_path = J('logs.txt')

        c = ccollections.Collections(r = run, p = progress)
        c.populate_collections_dict(input_files.profile_db)

        source_collections = set(map(str.strip, args.source_collections.split(',')))

        missing_collections = source_collections - set(c.collections_dict.keys())

        if len(missing_collections):
            raise ConfigError("Some of the collections you wanted are missing in the database. "
                             "Here is the list of missing collections: %s" % (", ".join(missing_collections)))

        c_names = []
        c_files = []

        for collection_name in source_collections:
            prefix = J(collection_name)

            c_names.append(collection_name)
            c_files.append(prefix + '.txt')

            c.export_collection(collection_name, output_file_prefix=prefix, include_unbinned=False)

        cmd_line = [self.program_name,
            '-c', input_files.splits_fasta,
            '-i', ','.join(c_files),
            '-l', ','.join(c_names),
            '-o', J('OUTPUT'),
            '--threads', str(threads),
            *utils.serialize_args(args,
                use_underscore=True,
                skip_keys=['source_collections'])]

        self.progress.new(self.program_name)
        self.progress.update('Running using %d threads...' % threads)
        utils.run_command(cmd_line, log_path)
        self.progress.end()

        output_file_name = 'OUTPUT_DASTool_scaffolds2bin.txt'
        output_file_path = J(output_file_name)
        if not os.path.exists(output_file_path):
            raise ConfigError("One of the critical output files is missing ('%s'). Please take a look at the "
                              "log file: %s" % (output_file_name, log_path))

        clusters = {}
        with open(output_file_path, 'r') as f:
            lines = f.readlines()

            for entry in lines:
                contig, bin_name = map(str.strip, entry.split())

                pretty_bin_name = 'Bin_' + bin_name.replace('.', '_')

                if pretty_bin_name not in clusters:
                    clusters[pretty_bin_name] = []

                clusters[pretty_bin_name].append(contig)

        # restore cwd
        os.chdir(cwd_backup)

        return clusters
